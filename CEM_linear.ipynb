{"cells":[{"cell_type":"markdown","source":[" How to train an AI to balance the cart pole\n"," The code is adapted from [Here](https://github.com/udacity/deep-reinforcement-learning/blob/master/cross-entropy/CEM.ipynb)."],"metadata":{}},{"cell_type":"code","execution_count":1,"source":["import numpy as np\n","import gym\n","import pandas as pd\n","\n","\n","env = gym.make(\"CartPole-v1\")\n","\n","total_reward = 0.0\n","total_steps = 0\n","obs = env.reset()\n"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":[" Read more about the gym environment [HERE](https://gym.openai.com/envs/CartPole-v1/)"],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["\n","while True:\n","    action = env.action_space.sample()\n","    env.render()\n","    obs, reward, done, _ = env.step(action)\n","    total_reward += reward\n","    total_steps += 1\n","    if done:\n","        break\n","\n","print(\"Episode done in %d steps, total reward %.2f\" %\n","      (total_steps, total_reward))\n","\n"],"outputs":[{"output_type":"stream","name":"stdout","text":["Episode done in 34 steps, total reward 34.00\n"]}],"metadata":{}},{"cell_type":"markdown","source":["![rewardCurve](<./rewardCurve.png>)\n","<img src=\"rewardCurve.png\" alt=\"rewardCurve\" style=\"zoom:40%;\" />\n"],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["class LinearPolicy(object):\n","\n","    def __init__(self, theta, ob_space, ac_space):\n","        \"\"\"\n","        dim_ob: dimension of observations\n","        n_actions: number of actions\n","        theta: flat vector of parameters\n","        \"\"\"\n","        dim_ob = ob_space.shape[0]\n","        n_actions = ac_space.n\n","        assert len(theta) == (dim_ob + 1) * n_actions\n","        self.W = theta[0: dim_ob * n_actions].reshape(dim_ob, n_actions)\n","        self.b = theta[dim_ob * n_actions: None].reshape(1, n_actions)\n","\n","    def act(self, ob):\n","        y = ob.dot(self.W) + self.b\n","        a = y.argmax()\n","        return a\n","\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["def run_episode(policy, env, num_steps, render=False):\n","    total_rew = 0\n","    ob = env.reset()\n","    for t in range(num_steps):\n","        a = policy.act(ob)\n","        (ob, reward, done, _info) = env.step(a)\n","        total_rew += reward\n","        if render and t % 3 == 0:\n","            env.render()\n","        if done:\n","            break\n","\n","    return total_rew\n","\n","\n","dim_theta = (env.observation_space.shape[0]+1) * env.action_space.n\n","print(\"observation space:\", env.observation_space.shape)\n","print(\"action space:\", env.action_space.n)\n","print(\"parameter number:\", dim_theta)\n","\n","\n","# Initialize mean and standard deviation\n","theta_mean = np.zeros(dim_theta)\n","theta_std = np.ones(dim_theta)\n","popsize = 25\n","n_elite = 5\n","num_steps = 500\n","\n","\n","reward_list = []\n","\n","for itr in range(50):\n","    # Sample parameter vectors\n","    thetas = np.random.multivariate_normal(mean=theta_mean,\n","                                           cov=np.diag(np.array(theta_std**2)),\n","                                           size=popsize)\n","    rewards = []\n","    for theta in thetas:\n","        policy = LinearPolicy(theta, env.observation_space, env.action_space)\n","        r = run_episode(policy, env, num_steps)\n","        rewards.append(r)\n","\n","    rewards = np.array(rewards)\n","    # Get elite parameters\n","    elite_inds = rewards.argsort()[-n_elite:]\n","    elite_thetas = thetas[elite_inds]\n","\n","    # Update theta_mean, theta_std\n","    theta_mean = elite_thetas.mean(axis=0)\n","    theta_std = elite_thetas.std(axis=0)\n","    print(\"[Iteration %2i] mean: %5.3g max: %5.3g\" %\n","          (itr, np.mean(rewards), np.max(rewards)))\n","    reward_list.append(np.mean(rewards))\n","    policy = LinearPolicy(theta_mean, env.observation_space, env.action_space)\n","    # run_episode(policy, env, num_steps, render=True)\n","\n","df = pd.DataFrame({\"reward\": reward_list})\n","df.to_csv(\"./CemLabData/results.csv\",\n","          index=False, header=True)\n"],"outputs":[{"output_type":"stream","name":"stdout","text":["observation space: (4,)\n","action space: 2\n","parameter number: 10\n","[Iteration  0] mean:  17.1 max:    60\n","[Iteration  1] mean:  44.8 max:   149\n","[Iteration  2] mean:  78.6 max:   315\n","[Iteration  3] mean:   134 max:   456\n","[Iteration  4] mean:   236 max:   500\n","[Iteration  5] mean:   343 max:   500\n","[Iteration  6] mean:   417 max:   500\n","[Iteration  7] mean:   474 max:   500\n","[Iteration  8] mean:   487 max:   500\n","[Iteration  9] mean:   482 max:   500\n","[Iteration 10] mean:   492 max:   500\n","[Iteration 11] mean:   499 max:   500\n","[Iteration 12] mean:   500 max:   500\n","[Iteration 13] mean:   500 max:   500\n","[Iteration 14] mean:   500 max:   500\n","[Iteration 15] mean:   500 max:   500\n","[Iteration 16] mean:   500 max:   500\n","[Iteration 17] mean:   500 max:   500\n","[Iteration 18] mean:   500 max:   500\n","[Iteration 19] mean:   500 max:   500\n","[Iteration 20] mean:   500 max:   500\n","[Iteration 21] mean:   500 max:   500\n","[Iteration 22] mean:   500 max:   500\n","[Iteration 23] mean:   500 max:   500\n","[Iteration 24] mean:   500 max:   500\n","[Iteration 25] mean:   500 max:   500\n","[Iteration 26] mean:   500 max:   500\n","[Iteration 27] mean:   500 max:   500\n","[Iteration 28] mean:   500 max:   500\n","[Iteration 29] mean:   500 max:   500\n","[Iteration 30] mean:   500 max:   500\n","[Iteration 31] mean:   500 max:   500\n","[Iteration 32] mean:   500 max:   500\n","[Iteration 33] mean:   500 max:   500\n","[Iteration 34] mean:   500 max:   500\n","[Iteration 35] mean:   500 max:   500\n","[Iteration 36] mean:   500 max:   500\n","[Iteration 37] mean:   500 max:   500\n","[Iteration 38] mean:   500 max:   500\n","[Iteration 39] mean:   500 max:   500\n","[Iteration 40] mean:   500 max:   500\n","[Iteration 41] mean:   500 max:   500\n","[Iteration 42] mean:   500 max:   500\n","[Iteration 43] mean:   500 max:   500\n","[Iteration 44] mean:   500 max:   500\n","[Iteration 45] mean:   500 max:   500\n","[Iteration 46] mean:   500 max:   500\n","[Iteration 47] mean:   500 max:   500\n","[Iteration 48] mean:   500 max:   500\n","[Iteration 49] mean:   500 max:   500\n"]}],"metadata":{}},{"cell_type":"markdown","source":[],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"},"orig_nbformat":4,"kernelspec":{"name":"python3","display_name":"Python 3.8.11 64-bit ('drl': conda)"},"interpreter":{"hash":"0fd1b3d1410010e491b0a7c2bc69bcfc8d835dc212573f86a56128d8622d672b"}}}